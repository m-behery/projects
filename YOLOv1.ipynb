{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5a34dad-b68c-4e30-8464-02237b2c57fd",
   "metadata": {},
   "source": [
    "# **YOLO (version 1) Implementation (from scratch):**\n",
    "\n",
    "***author:** Mohamed Behery (with special thanks to <a href=\"https://www.youtube.com/@AladdinPersson\">Aladdin Persson</a>)*<br/>\n",
    "***email:** m.behery@live.com*<br/>\n",
    "***Phone/Skype Username:** +201062989114*\n",
    "\n",
    "## **Implementation Methodology:**\n",
    "\n",
    "This code was developed after following the details and explanations provided by <a href=\"https://www.youtube.com/@AladdinPersson\">Aladdin Persson</a> in the following youtube video:\n",
    "\n",
    "**Title:** YOLOv1 from Scratch<br/>\n",
    "**URL:**   *https://www.youtube.com/watch?v=n9_XyCGr-MI*\n",
    "\n",
    "<font color = 'green'>**As a contribution**</font> to Aladdin's job that was <font color = 'red'>**amazingly**</font> done, several aspects of improvement were applied:\n",
    ">- It implements the entire algorithm including fetching datasets, preprocessing and training the model in this file alone. Where the dataset subsetting and preparation phase was redeveloped to have a collective summarization algorithm implemented for inspecting the datapoints available for each class and for each dataset, so that other versions of the VOC datasets are to be downloaded for the algorithm to combine them accordingly.\n",
    "\n",
    ">- It doesn't update, in place, the objects used in calculating the cost function because it leads to broken links in the graph when performing gradient descent. Although this doesn't show in Aladdin's video, where I might have missed something.\n",
    "\n",
    ">- It uses a much more concise form that allows changing the number of classes, cells and bounding boxes per cell which is reflected in calculating the model's cost function making it easy to overhaul the entire architecture to the next level.\n",
    "\n",
    "## **Conclusion:**\n",
    "\n",
    "Implementing the algorithm requires a large number of CUDA cores for faster training times which is not available with my current local GPU (RTX 2060) and therefore training, validation and most definitely testing cannot be seen through. However, the code should run well. If you find something to be tweaked, kindly DM me over Whatsapp/Skype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a498a7df-424f-4072-927d-d2392c7e5997",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "from functools import reduce\n",
    "from torchvision.io import read_image\n",
    "from torchvision.transforms import v2\n",
    "from xml.etree import ElementTree as ET\n",
    "from os.path import isfile, dirname, join\n",
    "from os import listdir, makedirs, getcwd\n",
    "import pandas as pd\n",
    "from nvidia_smi import nvmlInit, nvmlShutdown, nvmlDeviceGetHandleByIndex as nvmlGetHandle, nvmlDeviceGetMemoryInfo as nvmlGetMemory\n",
    "from psutil import virtual_memory\n",
    "\n",
    "class YOLOv1(nn.Module):\n",
    "\n",
    "    __LAYER_INFO = [\n",
    "        (64,7,2),\n",
    "        (None,2,2),\n",
    "        (192,3,1),\n",
    "        (None,2,2),\n",
    "        (128,1,1),\n",
    "        (256,3,1),\n",
    "        (256,1,1),\n",
    "        (512,3,1),\n",
    "        (None,2,2),\n",
    "        *[(256,1,1), (512,3,1)] * 4,\n",
    "        (512,1,1),\n",
    "        (1024,3,1),\n",
    "        (None,2,2),\n",
    "        *[(512,1,1), (1024,3,1)] * 2,\n",
    "        (1024,3,1),\n",
    "        (1024,3,2),\n",
    "        (1024,3,1),\n",
    "        (1024,3,1)\n",
    "    ]\n",
    "\n",
    "    def __init__(self, chw = (3, 448, 448), classes = 20, bboxes = 2):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.__DEVICE = 'cpu' if torch.cuda.is_available() else 'cpu'\n",
    "        \n",
    "        self.__C, self.__H, self.__W = chw\n",
    "        self.__S = reduce(lambda a, b: a * b, [x[-1] for x in self.__LAYER_INFO])\n",
    "        self.__CLASSES = classes\n",
    "        self.__BBOXES = bboxes\n",
    "        \n",
    "        assert self.__H % self.__S == 0 and self.__W % self.__S == 0, f'Image dimensions ({self.__H}, {self.__W}) must be divisible by the CNN stride ({self__S}).'\n",
    "        \n",
    "        self.__CELLS = self.__H * self.__W // self.__S ** 2\n",
    "\n",
    "        if len(self.__LAYER_INFO[0]) < 4:\n",
    "            self.__prepare_layer_info()\n",
    "        \n",
    "        self.darknet = nn.Sequential(*[self.cnn_unit(*x) for x in self.__LAYER_INFO])\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(1024 * self.__CELLS, 4096),\n",
    "            nn.Dropout(0.0),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(4096, self.__CELLS * (self.__CLASSES + 5 * self.__BBOXES))\n",
    "        )\n",
    "        \n",
    "        super().to(self.__DEVICE)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(self.darknet(x))\n",
    "    \n",
    "    def __prepare_layer_info(self):\n",
    "        prev_out_channels = self.__C\n",
    "        for i, x in enumerate(self.__LAYER_INFO):\n",
    "            self.__LAYER_INFO[i] = tuple([prev_out_channels if x[0] else None] + list(x))\n",
    "            if x[0]:\n",
    "                prev_out_channels = x[0]\n",
    "            \n",
    "    @staticmethod\n",
    "    def cnn_unit(*layer_args):\n",
    "        in_channels, out_channels, kernel, stride = layer_args\n",
    "        if in_channels:\n",
    "            padding = kernel // 2\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel, stride, padding, bias = False),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                nn.LeakyReLU(0.1)\n",
    "            )\n",
    "        return nn.MaxPool2d(kernel, stride)\n",
    "        \n",
    "    def chw(self):\n",
    "        return self.__C, self.__H, self.__W\n",
    "\n",
    "    def stride(self):\n",
    "        return self.__S\n",
    "\n",
    "    def cells(self):\n",
    "        return self.__CELLS\n",
    "\n",
    "    def layer_info(self):\n",
    "        return self.__LAYER_INFO\n",
    "\n",
    "    def device(self):\n",
    "        return self.__DEVICE\n",
    "\n",
    "    def classes(self):\n",
    "        return self.__CLASSES\n",
    "\n",
    "    def bboxes(self):\n",
    "        return self.__BBOXES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "718bfff7-04c7-467c-8317-a0931c51b3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class helper:\n",
    "\n",
    "    @staticmethod\n",
    "    def iou(bboxes_a, bboxes_b, fmt = 'midpoint'):\n",
    "    \n",
    "        assert fmt in ['midpoint', 'corners', 'hybrid']\n",
    "        \n",
    "        def reformat(bboxes):\n",
    "            if fmt == 'midpoint':\n",
    "                bboxes[..., :2] = bboxes[..., :2] - bboxes[..., 2:] / 2\n",
    "                bboxes[..., 2:] = bboxes[..., :2] + bboxes[..., 2:] / 2\n",
    "            elif fmt == 'hybrid':\n",
    "                bboxes[..., 2:] = bboxes[..., :2] + bboxes[..., 2:]\n",
    "            return bboxes\n",
    "        \n",
    "        def area(bboxes):\n",
    "            return ((bboxes[..., 0] - bboxes[..., 2]) * (bboxes[..., 1] - bboxes[..., 3])).abs()\n",
    "        \n",
    "        def intersection(bboxes_a, bboxes_b):\n",
    "            bboxes_ab = torch.cat([bboxes_a.unsqueeze(0), bboxes_b.unsqueeze(0)], dim = 0)\n",
    "            bboxes = torch.empty_like(bboxes_a)\n",
    "            bboxes[..., :2] = bboxes_ab[..., :2].max(dim = 0).values\n",
    "            bboxes[..., 2:] = bboxes_ab[..., 2:].min(dim = 0).values\n",
    "            return bboxes\n",
    "    \n",
    "        bboxes_a, bboxes_b = map(reformat, [bboxes_a, bboxes_b])\n",
    "        area_a, area_b = map(area, [bboxes_a, bboxes_b])\n",
    "        \n",
    "        inter_area_ab = area(intersection(bboxes_a, bboxes_b))\n",
    "        union_area_ab = (area_a + area_b) - inter_area_ab\n",
    "        \n",
    "        return inter_area_ab / (union_area_ab + 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ca274ab-e963-4fab-ac49-3f0c3ee11e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss(nn.Module):\n",
    "    \n",
    "    def __init__(self, model, coefs):\n",
    "        super().__init__()\n",
    "        _, self.__H, self.__W = model.chw()\n",
    "        self.__S = model.stride()\n",
    "        self.__BBOXES = model.bboxes()\n",
    "        self.__CLASSES = model.classes()\n",
    "        self.__DEVICE = model.device()\n",
    "        self.__COEF_NOOBJ, self.__COEF_COORD = coefs\n",
    "        self.mse = nn.MSELoss(reduction = 'sum')\n",
    "        super().to(self.__DEVICE)\n",
    "\n",
    "    def bboxes(y, h, w, d):\n",
    "        return y.reshape(-1, h, w, d)\n",
    "        y_pred.reshape(-1, self.__H // self.__S, self.__W // self.__S, self.__CLASSES + 5 * self.__BBOXES)\n",
    "        \n",
    "    def forward(self, y_pred, y_true):\n",
    "        \n",
    "        y_pred, y_true = map(lambda y: y.reshape(-1, self.__H // self.__S, self.__W // self.__S, self.__CLASSES + 5 * self.__BBOXES), [y_pred, y_true])\n",
    "        \n",
    "        bbox_slice = lambda idx: slice(self.__CLASSES + 5 * idx + 1, self.__CLASSES + 5 * (idx + 1))\n",
    "        \n",
    "        _, best_idxs = torch.cat([helper.iou(y_pred[..., bbox_slice(i)], y_true[..., bbox_slice(0)]).unsqueeze(0) for i in range(self.__BBOXES)], dim = 0).max(0)\n",
    "        best_idxs = F.one_hot(best_idxs, self.__BBOXES)\n",
    "        \n",
    "        bboxes_pred = torch.cat([y_pred[..., bbox_slice(i)].unsqueeze(0) for i in range(self.__BBOXES)], dim = 0).permute(1,2,3,4,0)\n",
    "        best_bboxes_pred = (best_idxs.unsqueeze(-2) * bboxes_pred).sum(-1)\n",
    "        \n",
    "        bboxes_exist_true = y_true[..., self.__CLASSES].unsqueeze(-1)\n",
    "        \n",
    "        best_bboxes_pred = bboxes_exist_true * best_bboxes_pred\n",
    "        bboxes_true = bboxes_exist_true * y_true[..., bbox_slice(0)]\n",
    "\n",
    "        best_bboxes_pred__lw = torch.sign(best_bboxes_pred[..., 2:]) * torch.sqrt(best_bboxes_pred[..., 2:].abs() + 1e-6)\n",
    "        bboxes_true__lw = torch.sign(bboxes_true[..., 2:]) * torch.sqrt(bboxes_true[..., 2:].abs() + 1e-6)\n",
    "\n",
    "        best_bboxes_pred = torch.cat([best_bboxes_pred[..., :2], best_bboxes_pred__lw], dim = -1)\n",
    "        bboxes_true = torch.cat([bboxes_true[..., :2], bboxes_true__lw], dim = -1)\n",
    "        \n",
    "        bbox_loss = ((best_bboxes_pred - bboxes_true) ** 2).sum()\n",
    "\n",
    "        bboxes_exist_pred = y_pred[..., self.__CLASSES::5]\n",
    "        best_bboxes_exist_pred = (best_idxs * bboxes_exist_pred).sum(-1)\n",
    "\n",
    "        bboxes_exist_true = bboxes_exist_true.squeeze(-1)\n",
    "        obj_loss = ((bboxes_exist_true * best_bboxes_exist_pred - bboxes_exist_true) ** 2).sum()\n",
    "\n",
    "        bboxes_not_exist_true = (1 - bboxes_exist_true).unsqueeze(-1)\n",
    "        noobj_loss = ((bboxes_not_exist_true * bboxes_exist_pred - bboxes_not_exist_true) ** 2).sum()\n",
    "\n",
    "        bboxes_exist_true = bboxes_exist_true.unsqueeze(-1)\n",
    "        class_loss = ((bboxes_exist_true * y_pred[..., :self.__CLASSES] - bboxes_exist_true * y_true[..., :self.__CLASSES]) ** 2).sum()\n",
    "\n",
    "        return (self.__COEF_COORD * bbox_loss + obj_loss + self.__COEF_NOOBJ * noobj_loss + class_loss) / y_pred.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a70e2dcb-ea34-4968-828e-c34965029ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VOCSubsets:\n",
    "\n",
    "    def __init__(self, root_dirpath, difficult_allowed):\n",
    "        self.__ROOT = root_dirpath\n",
    "        self.__DIFF = difficult_allowed\n",
    "        self.__SUBS, self.__CLSS = self.build(self.__ROOT, self.__DIFF)\n",
    "    \n",
    "    @staticmethod\n",
    "    def parse_label(xml_path, difficult_allowed = True):        \n",
    "        with open(xml_path) as f:\n",
    "            root = ET.parse(f).getroot()\n",
    "        node = root.find('size')\n",
    "        W, H = map(lambda x: float(node.find(x).text), ['width', 'height'])\n",
    "        objs = []\n",
    "        for node in root.iter('object'):\n",
    "            is_difficult = int(node.find('difficult').text)\n",
    "            if not difficult_allowed and is_difficult:\n",
    "                continue\n",
    "            name = node.find('name').text\n",
    "            node = node.find('bndbox')\n",
    "            x1, x2, y1, y2 = [float(node.find(x).text) / (W if i < 2 else H) for i, x in enumerate(['xmin', 'xmax', 'ymin', 'ymax'])]\n",
    "            x, y = (x1 + x2) / 2, (y1 + y2) / 2\n",
    "            w, h = x2 - x1, y2 - y1\n",
    "            x, y, w, h = map(lambda v: round(v, 4), [x, y, w, h])\n",
    "            objs.append((name, x, y, w, h))\n",
    "        return objs\n",
    "\n",
    "    @staticmethod\n",
    "    def build(root_dirpath, difficult_allowed):\n",
    "        if root_dirpath.startswith('./'):\n",
    "            root_dirpath = getcwd() + '/' + root_dirpath[2:]\n",
    "        subset_keys = ('train', 'val', 'test')\n",
    "        subset_dirpaths = [join(root_dirpath, x) + '/ImageSets/Main' for x in listdir(root_dirpath) if x.startswith('VOC')]\n",
    "        subset_filepaths = { y : [f'{x}/{y}.txt' for x in subset_dirpaths if isfile(f'{x}/{y}.txt')] for y in subset_keys }\n",
    "        subsets, classes = dict(), set()\n",
    "        for key in subset_keys:\n",
    "            subsets[key] = []\n",
    "            for path in subset_filepaths[key]:\n",
    "                image_dirpath, label_dirpath = [f'{dirname(dirname(dirname(path)))}/{x}' for x in ('JPEGImages', 'Annotations')]\n",
    "                with open(path) as f:\n",
    "                    lines = filter(lambda x: x.strip(), f.readlines())\n",
    "                for line in lines:\n",
    "                    image_path = f'{image_dirpath}/{line.strip()}.jpg'\n",
    "                    objs = VOCSubsets.parse_label(f'{label_dirpath}/{line.strip()}.xml', difficult_allowed)\n",
    "                    classes.update([x[0] for x in objs])\n",
    "                    label = '|'.join([','.join([str(x) for x in obj]) for obj in objs])\n",
    "                    subsets[key].append(f'{image_path} {label}')\n",
    "            dst_dirpath = join(root_dirpath, 'Subsets')\n",
    "            makedirs(dst_dirpath, exist_ok = True)\n",
    "            dst = f'{dst_dirpath}/{key}.txt'\n",
    "            with open(dst, 'w') as f:\n",
    "                f.write('\\n'.join(subsets[key]) + '\\n')\n",
    "            subsets[key] = dst\n",
    "        return subsets, sorted(classes)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.__SUBS[item]\n",
    "\n",
    "    def difficult_allowed(self):\n",
    "        return self.__DIFF\n",
    "\n",
    "    def root_dirpath(self):\n",
    "        return self.__ROOT\n",
    "\n",
    "    def subsets(self):\n",
    "        return self.__SUBS\n",
    "\n",
    "    def classes(self):\n",
    "        return self.__CLSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "520a44c7-fdaa-4c52-8d89-2f9a6c2c9090",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VOCDataset(Dataset):\n",
    "\n",
    "    def __init__(self, src_filepath, chw, classes, divs, bboxes):\n",
    "        with open(src_filepath) as f:\n",
    "            self.__IMAGES, self.__LABELS = zip(*[x.split() for x in f.readlines() if x.strip()])\n",
    "        self.__CLASSES = classes\n",
    "        self.__DIVS = divs\n",
    "        self.__BBOXES = bboxes\n",
    "        self.__COUNTS = ['\\n'.join(self.__LABELS).count(x) for x in self.__CLASSES]\n",
    "        self.__CHW = chw\n",
    "        self.__PREPROCESS = v2.Compose([\n",
    "            v2.Resize(size = self.__CHW[1:], antialias = True),\n",
    "            v2.Normalize(mean = [0.0] * 3, std = [1.0] * 3)\n",
    "        ])\n",
    "    \n",
    "    def formulate(self, label):        \n",
    "        objs = [[float(y) if j > 0 else y for j, y in enumerate(x.split(','))] for x in label.split('|')]\n",
    "        cell_index = lambda pos: int(pos * self.__DIVS) - 1\n",
    "        n_classes = len(self.__CLASSES)\n",
    "        label = torch.zeros(self.__DIVS, self.__DIVS, n_classes + 5 * self.__BBOXES)\n",
    "        for obj in objs:\n",
    "            name, x, y, w, h = obj\n",
    "            class_id = self.__CLASSES.index(name)\n",
    "            i, j = map(cell_index, [x, y])\n",
    "            label[i, j, class_id] = 1\n",
    "            label[i, j, n_classes : n_classes + 5] = torch.tensor([1, x, y, w, h])\n",
    "        return label.flatten()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.__IMAGES)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        image = self.__PREPROCESS(read_image(self.__IMAGES[item]) / 255)\n",
    "        label = self.formulate(self.__LABELS[item])\n",
    "        return image, label\n",
    "    \n",
    "    def summary(self):\n",
    "        series = pd.Series(self.__COUNTS, index = self.__CLASSES, name = 'count')\n",
    "        series[''] = series.sum()\n",
    "        df = series.reset_index()\n",
    "        df.rename(columns = {'index':'class'}, index = {len(self.__CLASSES):''}, inplace = True)\n",
    "        return df\n",
    "    \n",
    "    @staticmethod\n",
    "    def collective_summary(**datasets):\n",
    "        summary = pd.concat([x.summary().set_index('class') for x in datasets.values()], axis = 1)\n",
    "        summary.columns = [x.upper() for x in datasets.keys()]\n",
    "        summary[''] = summary.sum(axis = 1)\n",
    "        return summary.reset_index().rename(index = {summary.shape[0]-1:''})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "717692a0-5409-42f8-8fad-e6e024e60ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ram_used(device):\n",
    "    assert device in {'cpu', 'cuda'}\n",
    "    if device == 'cpu':\n",
    "        return virtual_memory()[2]/100\n",
    "    else:\n",
    "        info = nvmlGetMemory(nvmlGetHandle(0))\n",
    "        return info.used / info.total\n",
    "    \n",
    "def train(model, criterion, optimizer, dataloaders, epochs, best_weights):\n",
    "    \n",
    "    torch.autograd.set_detect_anomaly(True)\n",
    "    \n",
    "    def load_best():\n",
    "        if isfile(best_weights):\n",
    "            model.load_state_dict(torch.load(best_weights))\n",
    "        return model\n",
    "    \n",
    "    model = load_best()\n",
    "    device = model.device()\n",
    "    \n",
    "    if device == 'cuda':\n",
    "        nvmlInit()\n",
    "        \n",
    "    train_batches, val_batches = map(len, dataloaders)\n",
    "\n",
    "    flag = val_batches > train_batches\n",
    "    factor = val_batches // train_batches if flag else train_batches // val_batches\n",
    "\n",
    "    train_cost, val_cost = 0.0, 0.0\n",
    "    train_loader, val_loader = dataloaders\n",
    "\n",
    "    min_cost = 0.0\n",
    "    for epoch_idx in range(epochs):\n",
    "        \n",
    "        val_idx = 0\n",
    "        for train_idx, (x_train, y_train) in enumerate(train_loader):\n",
    "\n",
    "            try:\n",
    "                \n",
    "                model.train()\n",
    "                y_train = y_train.to(device)\n",
    "                batch_cost = criterion(model(x_train), y_train)\n",
    "                train_cost += batch_cost.item()\n",
    "                \n",
    "                val_flag = flag or (not flag and ((train_idx + 1) % factor == 0 or (train_idx + 1) == train_batches))\n",
    "                if val_flag:\n",
    "                    with torch.no_grad():\n",
    "                        for _ in range(factor if flag else 1):\n",
    "                            x_val, y_val = next(val_loader)\n",
    "                            val_idx += 1\n",
    "                            y_val = y_val.to(device)\n",
    "                            val_cost += criterion(model(x_val), y_val).item()\n",
    "                            print(f'\\rMemory [{get_ram_used(device):.1%}] - Epoch [{epoch_idx + 1}/{epochs}] - Train Batch [{train_idx + 1}/{train_batches}] - Valid Batch [{val_idx + 1}/{val_batches}]:\\tTrain Cost: {train_cost:.05f} | Valid Cost: {val_cost:.05f}', end = '')\n",
    "                print(f'\\rMemory [{get_ram_used(device):.1%}] - Epoch [{epoch_idx + 1}/{epochs}] - Train Batch [{train_idx + 1}/{train_batches}] - Valid Batch [{val_idx + 1}/{val_batches}]:\\tTrain Cost: {train_cost:.05f} | Valid Cost: {val_cost:.05f}', end = '')\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                batch_cost.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            except KeyboardInterrupt:\n",
    "                if device == 'cuda':\n",
    "                    nvmlShutdown()\n",
    "                    \n",
    "                return load_best()\n",
    "                \n",
    "\n",
    "        if val_cost < min_cost:\n",
    "            torch.save(model.state_dict(), best_weights)\n",
    "            min_cost = val_cost\n",
    "    \n",
    "    if device == 'cuda':\n",
    "        nvmlShutdown()\n",
    "        \n",
    "    return load_best() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99725967-e34f-4074-999c-11a877b8eba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%mkdir ./data\n",
    "\n",
    "!wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar -P ./data\n",
    "!tar xf ./data/VOCtrainval_06-Nov-2007.tar -C ./data\n",
    "\n",
    "!wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar -P ./data\n",
    "!tar xf ./data/VOCtest_06-Nov-2007.tar -C ./data\n",
    "\n",
    "!wget http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar -P ./data\n",
    "!tar xf ./data/VOCtrainval_11-May-2012.tar -C ./data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8e0933a-9609-4453-9142-11a7ac2ca196",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>TRAIN</th>\n",
       "      <th>VAL</th>\n",
       "      <th>TEST</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aeroplane</td>\n",
       "      <td>626</td>\n",
       "      <td>659</td>\n",
       "      <td>311</td>\n",
       "      <td>1596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bicycle</td>\n",
       "      <td>612</td>\n",
       "      <td>596</td>\n",
       "      <td>389</td>\n",
       "      <td>1597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bird</td>\n",
       "      <td>886</td>\n",
       "      <td>934</td>\n",
       "      <td>576</td>\n",
       "      <td>2396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>boat</td>\n",
       "      <td>716</td>\n",
       "      <td>681</td>\n",
       "      <td>393</td>\n",
       "      <td>1790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bottle</td>\n",
       "      <td>1087</td>\n",
       "      <td>1029</td>\n",
       "      <td>657</td>\n",
       "      <td>2773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>bus</td>\n",
       "      <td>448</td>\n",
       "      <td>461</td>\n",
       "      <td>254</td>\n",
       "      <td>1163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>car</td>\n",
       "      <td>2017</td>\n",
       "      <td>1991</td>\n",
       "      <td>1541</td>\n",
       "      <td>5549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>cat</td>\n",
       "      <td>800</td>\n",
       "      <td>816</td>\n",
       "      <td>370</td>\n",
       "      <td>1986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>chair</td>\n",
       "      <td>2183</td>\n",
       "      <td>2155</td>\n",
       "      <td>1374</td>\n",
       "      <td>5712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>cow</td>\n",
       "      <td>540</td>\n",
       "      <td>518</td>\n",
       "      <td>329</td>\n",
       "      <td>1387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>diningtable</td>\n",
       "      <td>521</td>\n",
       "      <td>536</td>\n",
       "      <td>299</td>\n",
       "      <td>1356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>dog</td>\n",
       "      <td>1039</td>\n",
       "      <td>1040</td>\n",
       "      <td>530</td>\n",
       "      <td>2609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>horse</td>\n",
       "      <td>584</td>\n",
       "      <td>572</td>\n",
       "      <td>395</td>\n",
       "      <td>1551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>motorbike</td>\n",
       "      <td>568</td>\n",
       "      <td>573</td>\n",
       "      <td>369</td>\n",
       "      <td>1510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>person</td>\n",
       "      <td>7724</td>\n",
       "      <td>7852</td>\n",
       "      <td>5227</td>\n",
       "      <td>20803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>pottedplant</td>\n",
       "      <td>862</td>\n",
       "      <td>862</td>\n",
       "      <td>592</td>\n",
       "      <td>2316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>sheep</td>\n",
       "      <td>700</td>\n",
       "      <td>647</td>\n",
       "      <td>311</td>\n",
       "      <td>1658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>sofa</td>\n",
       "      <td>617</td>\n",
       "      <td>594</td>\n",
       "      <td>396</td>\n",
       "      <td>1607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>train</td>\n",
       "      <td>485</td>\n",
       "      <td>499</td>\n",
       "      <td>302</td>\n",
       "      <td>1286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>tvmonitor</td>\n",
       "      <td>603</td>\n",
       "      <td>590</td>\n",
       "      <td>361</td>\n",
       "      <td>1554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td></td>\n",
       "      <td>23618</td>\n",
       "      <td>23605</td>\n",
       "      <td>14976</td>\n",
       "      <td>62199</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          class  TRAIN    VAL   TEST       \n",
       "0     aeroplane    626    659    311   1596\n",
       "1       bicycle    612    596    389   1597\n",
       "2          bird    886    934    576   2396\n",
       "3          boat    716    681    393   1790\n",
       "4        bottle   1087   1029    657   2773\n",
       "5           bus    448    461    254   1163\n",
       "6           car   2017   1991   1541   5549\n",
       "7           cat    800    816    370   1986\n",
       "8         chair   2183   2155   1374   5712\n",
       "9           cow    540    518    329   1387\n",
       "10  diningtable    521    536    299   1356\n",
       "11          dog   1039   1040    530   2609\n",
       "12        horse    584    572    395   1551\n",
       "13    motorbike    568    573    369   1510\n",
       "14       person   7724   7852   5227  20803\n",
       "15  pottedplant    862    862    592   2316\n",
       "16        sheep    700    647    311   1658\n",
       "17         sofa    617    594    396   1607\n",
       "18        train    485    499    302   1286\n",
       "19    tvmonitor    603    590    361   1554\n",
       "                 23618  23605  14976  62199"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "subsets = VOCSubsets('./data/VOCdevkit/', difficult_allowed = True)\n",
    "datasets = { x : VOCDataset(subsets[x], (3, 448, 448), subsets.classes(), 7, 2) for x in ['train', 'val', 'test'] }\n",
    "display(VOCDataset.collective_summary(**datasets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd64eea7-a7be-4f88-a19f-749d8a898e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory [46.1%] - Epoch [1/100] - Train Batch [1369/8218] - Valid Batch [1370/8333]:\tTrain Cost: 16293.56092 | Valid Cost: 15663.66891"
     ]
    }
   ],
   "source": [
    "dataloaders = {x : iter(DataLoader(dataset = datasets[x], batch_size = 1, shuffle = True)) for x in datasets}\n",
    "model = YOLOv1((3, 448, 448), 20, 2)\n",
    "loss = Loss(model, coefs = (0.5, 5))\n",
    "optimizer = Adam(params = model.parameters(), lr = 2e-5, weight_decay = 0.0)\n",
    "train(model, loss, optimizer, list(dataloaders.values())[:-1], 100, './best_weights.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
